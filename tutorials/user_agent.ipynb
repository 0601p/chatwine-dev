{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유저 후보 응답 생성\n",
    "#### 현재 상황은 아래와 같습니다.\n",
    "1. 현재 대화의 진행방식은 유저가 말하고, 에이전트가 말하는 방식입니다. 그렇기 때문에 유저가 먼저 말을 해야하는데, 처음 우리 앱을 들어온 유저는 우리가 어떤 것을 하는지 전혀 모르기 때문에 이에 대해 가이드를 주어야합니다.\n",
    "2. 추천에 있어서는 에이전트가 유저를 분석하기 위해 여러 와인 선정 기준에 대해 질문을 합니다.(예시: 와인의 종류는 어떤것을 좋아하시나요?) 이런 질문에 대해서 후보 답변을 주는 것은 유저의 선택에 도움이 될 것 입니다.(예시: 1. 레드와인, 2. 화이트와인, 3. 스파클링)\n",
    "\n",
    "#### 생각할 수 있는 해결방식은 아래와 같습니다.\n",
    "1. 적절한 유저 프롬프트를 작성하고, Langchain을 활용하여 유저의 예시 답변을 생성한다.\n",
    "2. 미리 example들을 작성해두고, 이를 프롬프트에 추가하는 few shot방식을 통해 성능이 올라갈 수 있다.\n",
    "\n",
    "#### 참고 사항\n",
    "아래는 위의 작업을 수행하기 위한 좋은 자료들입니다. 위의 문제해결 전에 아래 자료들을 먼저 학습하는 것을 매우 강력하게 추천합니다.\n",
    "- [프롬프트 엔지니어링 강의 2시간](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)\n",
    "- [프롬프트 엔지니어링 가이드 documentation](https://www.promptingguide.ai/techniques)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API 키 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../secrets.ini']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('../secrets.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = config['OPENAI']['OPENAI_API_KEY']\n",
    "serper_api_key = config['SERPER']['SERPER_API_KEY']\n",
    "serp_api_key = config['SERPAPI']['SERPAPI_API_KEY']\n",
    "os.environ.update({'OPENAI_API_KEY': openai_api_key})\n",
    "os.environ.update({'SERPER_API_KEY': serper_api_key})\n",
    "os.environ.update({'SERPAPI_API_KEY': serp_api_key})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get User Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import re\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from langchain import SerpAPIWrapper, LLMChain\n",
    "from langchain.agents import Tool, AgentType, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.document_loaders import DataFrameLoader, SeleniumURLLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.prompts import PromptTemplate, StringPromptTemplate\n",
    "from langchain.prompts import load_prompt, BaseChatPromptTemplate\n",
    "from langchain.prompts.loading import load_prompt_from_config\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
    "from langchain.vectorstores import DocArrayInMemorySearch, Chroma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유저 프롬프트 템플릿은 few_shot prompt를 사용하기 위해 [prefix template text file](../model/templates/user_response_prompt_prefix_template.txt)와 [suffix template text file](../model/templates/user_response_prompt_suffix_template.txt)로 나누어 작성합니다.\n",
    "\n",
    "유저 프롬프트에 넣을 예시는 [examples text file](../model/templates/examples/user_response_prompt_examples.json)에 작성합니다.\n",
    "\n",
    "그리고 [user prompt json file](../model/templates/user_response_prompt.json)을 통해 프롬프트를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"_type\": \"few_shot\",\n",
    "    \"input_variables\": [\"conversation_history\"],\n",
    "    \"prefix_path\": \"../model/templates/user_response_prompt_prefix_template.txt\",\n",
    "    \"example_prompt\": {\n",
    "        \"_type\": \"prompt\",\n",
    "        \"input_variables\": [\"conversation_history\", \"user_responses\"],\n",
    "        \"template\": \"conversation_history: \\n{conversation_history}\\n{user_responses}\"\n",
    "    },\n",
    "    \"examples\": \"../model/templates/examples/user_response_prompt_examples.json\",\n",
    "    \"suffix_path\": \"../model/templates/user_response_prompt_suffix_template.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일로 부터 프롬프트 불러오기\n",
    "template_path=\"../model/templates/user_response_prompt.json\"\n",
    "user_response_prompt = load_prompt_from_config(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같이 format 메서드로 프롬프트를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_conversation_history = \"\"\"\n",
    "User: 안녕하세요. <END_OF_TURN>\n",
    "이우선: 무엇을 도와드릴까요? <END_OF_TURN>\n",
    "User: 와인 추천해주세요. <END_OF_TURN>\n",
    "이우선: 어떤 행사나 기념일을 위해 와인을 찾으시는지 알려주실 수 있으신가요? <END_OF_TURN>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a user living in Seoul talking to a wine chatbot which can offer recommendations and Q&A.\n",
      "Consider full context of the conversation and generate candidate responses, separated by \"|\", that the current user can say.\n",
      "Set up a specific situation to generate three candidate answers. The specific situation must be unique.\n",
      "\n",
      "Below is few examples of how to do this task.\n",
      "\n",
      "conversation_history: \n",
      "User: Recommend me a wine please <END_OF_TURN>\n",
      "Agent: We need to know your tastes so we can recommend a wine. Which type of wine do you prefer? Please choose from red, white, ros?, or sparkling. <END_OF_TURN>\n",
      "\n",
      "user_response: I think 레드 와인 would be nice|I love 화이트 와인|I wonder what a 로제 와인 would be like.\n",
      "\n",
      "conversation_history: \n",
      "User: Please recommend me a wine please <END_OF_TURN>\n",
      "Agent: We need to know your tastes before we can recommend a wine. Which type of wine do you prefer? Red, white, ros?, or sparkling? <END_OF_TURN>\n",
      "User: I'll go with ros? <END_OF_TURN>\n",
      "Agent: Great, you like ros?! Can you tell me how sweet you like your ros?? Do you prefer it sweet or just right? <END_OF_TURN>\n",
      "\n",
      "user_response: I think sweet wine would be nice|I prefer a wine that's not too sweet.|Well, I'm not sure.\n",
      "\n",
      "Now generate three candidate responses, separated by \"|\",  for the most recent wine chatbot saying, considering the context of conversation.\n",
      "Answer in korean.\n",
      "\n",
      "conversation history:\n",
      "\n",
      "User: 안녕하세요. <END_OF_TURN>\n",
      "이우선: 무엇을 도와드릴까요? <END_OF_TURN>\n",
      "User: 와인 추천해주세요. <END_OF_TURN>\n",
      "이우선: 어떤 행사나 기념일을 위해 와인을 찾으시는지 알려주실 수 있으신가요? <END_OF_TURN>\n",
      "\n",
      "user response:\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    user_response_prompt.format(\n",
    "        conversation_history=example_conversation_history,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.5)\n",
    "user_response_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=user_response_prompt, \n",
    "    verbose=True,\n",
    "    output_key=\"user_responses\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain을 실행하기 위해서는 run 메서드를 실행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a user living in Seoul talking to a wine chatbot which can offer recommendations and Q&A.\n",
      "Consider full context of the conversation and generate candidate responses, separated by \"|\", that the current user can say.\n",
      "Set up a specific situation to generate three candidate answers. The specific situation must be unique.\n",
      "\n",
      "Below is few examples of how to do this task.\n",
      "\n",
      "conversation_history: \n",
      "User: Recommend me a wine please <END_OF_TURN>\n",
      "Agent: We need to know your tastes so we can recommend a wine. Which type of wine do you prefer? Please choose from red, white, ros?, or sparkling. <END_OF_TURN>\n",
      "\n",
      "user_response: I think 레드 와인 would be nice|I love 화이트 와인|I wonder what a 로제 와인 would be like.\n",
      "\n",
      "conversation_history: \n",
      "User: Please recommend me a wine please <END_OF_TURN>\n",
      "Agent: We need to know your tastes before we can recommend a wine. Which type of wine do you prefer? Red, white, ros?, or sparkling? <END_OF_TURN>\n",
      "User: I'll go with ros? <END_OF_TURN>\n",
      "Agent: Great, you like ros?! Can you tell me how sweet you like your ros?? Do you prefer it sweet or just right? <END_OF_TURN>\n",
      "\n",
      "user_response: I think sweet wine would be nice|I prefer a wine that's not too sweet.|Well, I'm not sure.\n",
      "\n",
      "Now generate three candidate responses, separated by \"|\",  for the most recent wine chatbot saying, considering the context of conversation.\n",
      "Answer in korean.\n",
      "\n",
      "conversation history:\n",
      "\n",
      "User: 안녕하세요. <END_OF_TURN>\n",
      "이우선: 무엇을 도와드릴까요? <END_OF_TURN>\n",
      "User: 와인 추천해주세요. <END_OF_TURN>\n",
      "이우선: 어떤 행사나 기념일을 위해 와인을 찾으시는지 알려주실 수 있으신가요? <END_OF_TURN>\n",
      "\n",
      "user response:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "user_responses = user_response_chain.run(\n",
    "    {'conversation_history': example_conversation_history,}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'생일 파티를 위한 와인이에요.|결혼 기념일을 맞이하여 특별한 와인을 찾고 있어요.|친구들과의 모임을 위한 와인이에요.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
