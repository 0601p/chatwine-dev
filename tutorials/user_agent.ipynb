{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이 문서를 수정할 당신에게...\n",
    "#### 현재 상황은 아래와 같습니다.\n",
    "1. 현재 대화의 진행방식은 유저가 말하고, 에이전트가 말하는 방식입니다. 그렇기 때문에 유저가 먼저 말을 해야하는데, 처음 우리 앱을 들어온 유저는 우리가 어떤 것을 하는지 전혀 모르기 때문에 이에 대해 가이드를 주어야합니다.\n",
    "2. 추천에 있어서는 에이전트가 유저를 분석하기 위해 여러 와인 선정 기준에 대해 질문을 합니다.(예시: 와인의 종류는 어떤것을 좋아하시나요?) 이런 질문에 대해서 후보 답변을 주는 것은 유저의 선택에 도움이 될 것 입니다.(예시: 1. 레드와인, 2. 화이트와인, 3. 스파클링)\n",
    "\n",
    "#### 당신의 목표는 아래와 같습니다.\n",
    "1. 유저가 실제로 우리앱을 쓰면서 대화 문맥 상에서 할만한 말들에 대한 후보를 생성합니다.\n",
    "\n",
    "#### 생각할 수 있는 해결방식은 아래와 같습니다.\n",
    "1. 적절한 유저 프롬프트를 작성하고, Langchain을 활용하여 유저의 예시 답변을 생성한다.\n",
    "2. 미리 example들을 작성해두고, 이를 프롬프트에 추가하는 few shot방식을 통해 성능이 올라갈 수 있다.\n",
    "\n",
    "#### 참고 사항\n",
    "아래는 위의 작업을 수행하기 위한 좋은 자료들입니다. 위의 문제해결 전에 아래 자료들을 먼저 학습하는 것을 매우 강력하게 추천합니다.\n",
    "- [프롬프트 엔지니어링 강의 2시간](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)\n",
    "- [프롬프트 엔지니어링 가이드 documentation](https://www.promptingguide.ai/techniques)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API 키 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./secrets.ini']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('./secrets.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = config['OPENAI']['OPENAI_API_KEY']\n",
    "serper_api_key = config['SERPER']['SERPER_API_KEY']\n",
    "serp_api_key = config['SERPAPI']['SERPAPI_API_KEY']\n",
    "os.environ.update({'OPENAI_API_KEY': openai_api_key})\n",
    "os.environ.update({'SERPER_API_KEY': serper_api_key})\n",
    "os.environ.update({'SERPAPI_API_KEY': serp_api_key})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get User Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import re\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from langchain import SerpAPIWrapper, LLMChain\n",
    "from langchain.agents import Tool, AgentType, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.document_loaders import DataFrameLoader, SeleniumURLLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.prompts import PromptTemplate, StringPromptTemplate\n",
    "from langchain.prompts import load_prompt, BaseChatPromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
    "from langchain.vectorstores import DocArrayInMemorySearch, Chroma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유저 프롬프트 템플릿은 few_shot prompt를 사용하기 위해 [prefix template text file](./templates/user_prompt_prefix_template.txt)와 [suffix template text file](./templates/user_prompt_suffix_template.txt)로 나누어 작성합니다.\n",
    "\n",
    "유저 프롬프트에 넣을 예시는 [examples text file](./templates/examples/user_prompt_examples.json)에 작성합니다.\n",
    "\n",
    "그리고 [user prompt json file](./templates/user_prompt_template.json)을 통해 프롬프트를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일로 부터 프롬프트 불러오기\n",
    "user_response_prompt = load_prompt(\"./templates/user_response_prompt.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같이 format 메서드로 프롬프트를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_conversation_history = \"\"\"\n",
    "User: 안녕하세요. <END_OF_TURN>\n",
    "이우선: 무엇을 도와드릴까요? <END_OF_TURN>\n",
    "User: 와인 추천해주세요. <END_OF_TURN>\n",
    "이우선: 어떤 행사나 기념일을 위해 와인을 찾으시는지 알려주실 수 있으신가요? <END_OF_TURN>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a user talking to a wine chatbot which can offer recommendations and Q&A.\n",
      "Read the conversation history and generate candidate responses for the current user to say.\n",
      "You can create as many candidate responses as you want.\n",
      "\n",
      "Below is few examples of how to do this task.\n",
      "\n",
      "conversation_history: \n",
      "\n",
      "user_response: 이번 주에 친구들과 모임이 있는데, 훌륭한 와인 한 병을 추천해줄래? | 입문자에게 좋은 와인을 추천해줄래? | 보르도와 부르고뉴 와인의 차이점은 뭐야?\n",
      "\n",
      "conversation_history: \n",
      "User: 와인 하나만 추천해줘 <END_OF_TURN>\n",
      "이우선: 고객님의 취향을 알아야 와인을 추천해드릴 수 있어요. 와인 종류 중 어떤 것을 선호하시나요? 레드, 화이트, 로제, 스파클링 중에서 선택해주세요. <END_OF_TURN>\n",
      "\n",
      "user_response: 레드 와인 | 화이트 와인 | 로제 와인 | 스파클링\n",
      "\n",
      "conversation_history: \n",
      "User: 와인 하나만 추천해주세요 <END_OF_TURN>\n",
      "이우선: 고객님의 취향을 알아야 와인을 추천해드릴 수 있어요. 와인 종류 중 어떤 것을 선호하시나요? 레드, 화이트, 로제, 스파클링 중에서 선택해주세요. <END_OF_TURN>\n",
      "User: 로제가 좋겠군 <END_OF_TURN>\n",
      "이우선: 좋아요, 로제 와인을 좋아하시는군요! 로제 와인의 단맛 정도를 알려주시겠어요? 달달한 것을 선호하시나요, 아니면 적당한 단맛을 선호하시나요? <END_OF_TURN>\n",
      "\n",
      "User response candidate: 달달한 와인 | 달지않은 와인 | 적당한 단맛의 와인 | 잘 모르겠어\n",
      "\n",
      "Now generate a few candidate responses for the current user to say.\n",
      "\n",
      "conversation history:\n",
      "\n",
      "User: 안녕하세요. <END_OF_TURN>\n",
      "이우선: 무엇을 도와드릴까요? <END_OF_TURN>\n",
      "User: 와인 추천해주세요. <END_OF_TURN>\n",
      "이우선: 어떤 행사나 기념일을 위해 와인을 찾으시는지 알려주실 수 있으신가요? <END_OF_TURN>\n",
      "\n",
      "user response:\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    user_response_prompt.format(\n",
    "        conversation_history=example_conversation_history,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랭체인 모델 선언, 랭체인은 언어모델과 프롬프트로 구성됩니다.\n",
    "llm = ChatOpenAI(model='gpt-4', temperature=0.5)\n",
    "user_response_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=user_response_prompt, \n",
    "    verbose=True, # 과정을 출력할지\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain을 실행하기 위해서는 run 메서드를 실행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a user talking to a wine chatbot which can offer recommendations and Q&A.\n",
      "Read the conversation history and generate candidate responses for the current user to say.\n",
      "You can create as many candidate responses as you want.\n",
      "\n",
      "Below is few examples of how to do this task.\n",
      "\n",
      "conversation_history: \n",
      "\n",
      "user_response: 이번 주에 친구들과 모임이 있는데, 훌륭한 와인 한 병을 추천해줄래? | 입문자에게 좋은 와인을 추천해줄래? | 보르도와 부르고뉴 와인의 차이점은 뭐야?\n",
      "\n",
      "conversation_history: \n",
      "User: 와인 하나만 추천해줘 <END_OF_TURN>\n",
      "이우선: 고객님의 취향을 알아야 와인을 추천해드릴 수 있어요. 와인 종류 중 어떤 것을 선호하시나요? 레드, 화이트, 로제, 스파클링 중에서 선택해주세요. <END_OF_TURN>\n",
      "\n",
      "user_response: 레드 와인 | 화이트 와인 | 로제 와인 | 스파클링\n",
      "\n",
      "conversation_history: \n",
      "User: 와인 하나만 추천해주세요 <END_OF_TURN>\n",
      "이우선: 고객님의 취향을 알아야 와인을 추천해드릴 수 있어요. 와인 종류 중 어떤 것을 선호하시나요? 레드, 화이트, 로제, 스파클링 중에서 선택해주세요. <END_OF_TURN>\n",
      "User: 로제가 좋겠군 <END_OF_TURN>\n",
      "이우선: 좋아요, 로제 와인을 좋아하시는군요! 로제 와인의 단맛 정도를 알려주시겠어요? 달달한 것을 선호하시나요, 아니면 적당한 단맛을 선호하시나요? <END_OF_TURN>\n",
      "\n",
      "User response candidate: 달달한 와인 | 달지않은 와인 | 적당한 단맛의 와인 | 잘 모르겠어\n",
      "\n",
      "Now generate a few candidate responses for the current user to say.\n",
      "\n",
      "conversation history:\n",
      "\n",
      "User: 안녕하세요. <END_OF_TURN>\n",
      "이우선: 무엇을 도와드릴까요? <END_OF_TURN>\n",
      "User: 와인 추천해주세요. <END_OF_TURN>\n",
      "이우선: 어떤 행사나 기념일을 위해 와인을 찾으시는지 알려주실 수 있으신가요? <END_OF_TURN>\n",
      "\n",
      "user response:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "user_responses = user_response_chain.run(\n",
    "    {'conversation_history': example_conversation_history,}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'생일 파티를 위한 와인 추천해주세요. | 결혼 기념일을 위한 와인 추천해주세요. | 친구와의 만남을 위한 와인 추천해주세요. | 저녁 식사를 위한 와인 추천해주세요.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
